import base64
import glob
import hashlib
import os
import shutil

import torch
from safetensors import safe_open
from safetensors.torch import save_file

# CID constants
CIDV1 = 0x01
RAW_CODEC = 0x55
SHA2_256_CODE = 0x12
SHA2_256_SIZE = 0x20  # 32 bytes


def calculate_cid_manual(data: bytes) -> str:
    sha256_digest = hashlib.sha256(data).digest()
    multihash = bytes([SHA2_256_CODE, SHA2_256_SIZE]) + sha256_digest
    cid_bytes = bytes([CIDV1, RAW_CODEC]) + multihash
    base32_str = base64.b32encode(cid_bytes).decode("ascii").lower().rstrip("=")
    cid_string = "b" + base32_str
    return cid_string


def inplace_insert_value_with_idx(tensor_list, value, idx):
    while len(tensor_list) < idx + 1:
        tensor_list.append(None)
    tensor_list[idx] = value


def concat_weight_partition(refit_weight_path):
    """
    Concat partial weight into one safetensor.
    Partitioned weight should be named in the following format:
    {original_name}_part{i}
    e.g. model.embed_tokens.weight_part0
    """
    weight_files = glob.glob(refit_weight_path + "/*.safetensors")
    assert weight_files, f"Weight safetensors files not found in path: {refit_weight_path}"

    tensors = {}
    original_tensors = {}
    for wf in weight_files:
        with safe_open(wf, framework="pt", device="cpu") as f:
            for k in f.keys():
                original_tensors[k] = f.get_tensor(k)
    for wf in weight_files:
        os.remove(wf)

    # Concatenate if needed and save the final tensors
    sorted_keys = sorted(original_tensors.keys())
    prev_key = None
    concate_list = []
    file_idx = 0
    max_size = 1024 * 1024 * 1024  # max size 1GB
    param_size = 0
    for key in sorted_keys:
        val = original_tensors[key]
        if "part" not in key:
            tensors[key] = val
            param_size += val.numel() * val.element_size()
            if param_size > max_size:
                save_file_name = refit_weight_path + "/model_" + str(file_idx) + ".safetensors"
                save_file(tensors, save_file_name)
                file_idx += 1
                param_size = 0
                tensors = {}
            continue

        name_split = key.split(".")
        cur_name_list = name_split[:-1]
        weight_name = name_split[-1]
        cur_idx = int(weight_name.removeprefix("weight_part"))
        if prev_key is None:
            inplace_insert_value_with_idx(concate_list, val, cur_idx)
            prev_key = key
        else:
            prev_name_list = prev_key.split(".")[:-1]
            if prev_name_list == cur_name_list:
                inplace_insert_value_with_idx(concate_list, val, cur_idx)
            else:
                concate_result = torch.cat(concate_list, 0)
                cur_name_list.append("weight")
                final_key = ".".join(cur_name_list)
                tensors[final_key] = concate_result
                param_size += val.numel() * val.element_size()
                if param_size > max_size:
                    save_file_name = refit_weight_path + "/model_" + str(file_idx) + ".safetensors"
                    save_file(tensors, save_file_name)
                    file_idx += 1
                    param_size = 0
                    tensors = {}

                # for next tensor
                concate_list = []
                inplace_insert_value_with_idx(concate_list, val, cur_idx)
            prev_key = key

    if concate_list:
        concate_result = torch.cat(concate_list, 0)
        cur_name_list = prev_key.split(".")[:-1]
        cur_name_list.append("weight")
        final_key = ".".join(cur_name_list)
        tensors[final_key] = concate_result

    save_file_name = refit_weight_path + "/model_" + str(file_idx) + ".safetensors"
    save_file(tensors, save_file_name)


def is_block_needed(key, is_first_shard, is_last_shard, start_layer, end_layer) -> bool:
    if is_first_shard and "embed_tokens" in key and key.startswith("model."):
        return True

    if is_last_shard:
        if "model.norm" in key or "lm_head" in key:
            return True
        if "embed" in key and key.startswith("model.embed_tokens"):
            return True

    if "layers." in key:
        parts = key.split(".")
        for i, part in enumerate(parts):
            if part == "layers" and i + 1 < len(parts):
                layer_idx = int(parts[i + 1])
                return start_layer <= layer_idx < end_layer

    return False


def filer_weight_cid_list(start_layer, end_layer, hidden_layers, index_map):
    """
    Filters block cids that worker node needs to download.
    Arguments:
        start_layer: local start layer
        end_layer: local end layer
        index_map:  Dict[str],  key(weight_name): value(cid)
    Returns:
        cid: List[int].  cid list that current worker node holds.
    """
    is_first_shard = start_layer == 0
    is_last_shard = end_layer == hidden_layers

    res = set()
    for key in index_map.keys():
        if is_block_needed(key, is_first_shard, is_last_shard, start_layer, end_layer):
            value = index_map.get(key)
            res.add(value)

    return list(res)


def remove_list_dirs(dir_list):
    for dir_path in dir_list:
        if os.path.isdir(dir_path):
            try:
                shutil.rmtree(dir_path)
            except OSError:
                pass
        else:
            continue


def release_disk_storage():
    """Remove lattica storage files before get blocks"""
    storage_dir = "/tmp"
    storage_dirs = glob.glob(storage_dir + "/*.storage")
    key_dirs = glob.glob(storage_dir + "/*.key")
    dht_dirs = glob.glob(storage_dir + "/*.dht")
    remove_list_dirs(storage_dirs)
    remove_list_dirs(key_dirs)
    remove_list_dirs(dht_dirs)
